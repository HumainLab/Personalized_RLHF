{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 92858\n",
      "eval_dataset size: 86086\n"
     ]
    }
   ],
   "source": [
    "use_downloads = False  # set this to False if the dataset is not downloaded to local disk\n",
    "\n",
    "if use_downloads:\n",
    "    downloads_data_path = \"your/downloaded/original/tldr/data/path\"\n",
    "    train_dataset = load_from_disk(os.path.join(downloads_data_path, \"train\"))\n",
    "    eval_dataset  = load_from_disk(os.path.join(downloads_data_path, \"validation\"))\n",
    "else:  \n",
    "    # download the openai/summarize_from_feedback dataset from huggingface\n",
    "    # the dataset will be saved to ~/.cache/huggingface/datasets/\n",
    "    train_dataset = load_dataset(\"openai/summarize_from_feedback\", name=\"comparisons\", split=\"train\")\n",
    "    eval_dataset  = load_dataset(\"openai/summarize_from_feedback\", name=\"comparisons\", split=\"validation\")\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset))\n",
    "print(\"eval_dataset size:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 38487\n",
      "eval_dataset size: 16294\n"
     ]
    }
   ],
   "source": [
    "# only use comparisons sampled from SFTs (but not ppo policies)\n",
    "filter_func = lambda x: all(y[\"policy\"].find(\"sup\") >= 0 and y[\"policy\"].find(\"ppo\") == -1 and \\\n",
    "                            y[\"policy\"].find(\"cnn\") == -1 for y in x[\"summaries\"])\n",
    "train_dataset = train_dataset.filter(filter_func)  \n",
    "eval_dataset  = eval_dataset.filter(filter_func)\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset))\n",
    "print(\"eval_dataset size:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation counts of each user\n",
    "train_user, train_user_cnt = np.unique(np.asarray(train_dataset[\"worker\"]), return_counts=True)\n",
    "train_user = pd.DataFrame(np.column_stack((train_user, train_user_cnt)), columns=[\"worker\", \"train_count\"])\n",
    "\n",
    "eval_user, eval_user_cnt = np.unique(np.asarray(eval_dataset[\"worker\"]), return_counts=True)\n",
    "eval_user = pd.DataFrame(np.column_stack((eval_user, eval_user_cnt)), columns=[\"worker\", \"eval_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank workers in descending order of the number of annotated comparisons\n",
    "user = pd.merge(left=train_user, right=eval_user, how=\"outer\", on=\"worker\")\n",
    "user.fillna(0, inplace=True)\n",
    "user[\"train_count\"] = user[\"train_count\"].astype(int)\n",
    "user[\"eval_count\"] = user[\"eval_count\"].astype(int)\n",
    "user = user.sort_values(by=[\"train_count\"], ascending=False)\n",
    "\n",
    "user[\"train_count_acum\"] = user[\"train_count\"].cumsum()\n",
    "user[\"eval_count_acum\"] = user[\"eval_count\"].cumsum()\n",
    "user = user.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>train_count</th>\n",
       "      <th>eval_count</th>\n",
       "      <th>train_count_acum</th>\n",
       "      <th>eval_count_acum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KZL1qeRzHNYSfDAuOctL1iyVV8WC5N</td>\n",
       "      <td>5245</td>\n",
       "      <td>1175</td>\n",
       "      <td>5245</td>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZzGCcAhvqF0HnKxNsUjtJFadcZdyZj</td>\n",
       "      <td>2920</td>\n",
       "      <td>643</td>\n",
       "      <td>8165</td>\n",
       "      <td>1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p4Oh7rUGyLe1EpilJFWr9sPDpkO016</td>\n",
       "      <td>2437</td>\n",
       "      <td>381</td>\n",
       "      <td>10602</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qo6WIyEh27cwAjWpA3Q60J7NaDxzQJ</td>\n",
       "      <td>2366</td>\n",
       "      <td>604</td>\n",
       "      <td>12968</td>\n",
       "      <td>2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zKV8BFGy60O0q7102ALF84S6Jo5i4q</td>\n",
       "      <td>2200</td>\n",
       "      <td>586</td>\n",
       "      <td>15168</td>\n",
       "      <td>3389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i8YiBZlrYmlkkChr5b9BUKvDO6lR1d</td>\n",
       "      <td>2126</td>\n",
       "      <td>360</td>\n",
       "      <td>17294</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M3icahkfAtC9CJrtKgQ7qvyZ5SD8wC</td>\n",
       "      <td>1824</td>\n",
       "      <td>321</td>\n",
       "      <td>19118</td>\n",
       "      <td>4070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HNzkrs9geGu1YMMfZ5Qvdt0ZaCthfB</td>\n",
       "      <td>1606</td>\n",
       "      <td>263</td>\n",
       "      <td>20724</td>\n",
       "      <td>4333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jxv4hxfb9zTVa5nsMDFlnjSX5LZ8MK</td>\n",
       "      <td>1366</td>\n",
       "      <td>177</td>\n",
       "      <td>22090</td>\n",
       "      <td>4510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UhQipwcpQmiGJmScocXOGOKyCBaFUg</td>\n",
       "      <td>1209</td>\n",
       "      <td>411</td>\n",
       "      <td>23299</td>\n",
       "      <td>4921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           worker  train_count  eval_count  train_count_acum  \\\n",
       "0  KZL1qeRzHNYSfDAuOctL1iyVV8WC5N         5245        1175              5245   \n",
       "1  ZzGCcAhvqF0HnKxNsUjtJFadcZdyZj         2920         643              8165   \n",
       "2  p4Oh7rUGyLe1EpilJFWr9sPDpkO016         2437         381             10602   \n",
       "3  qo6WIyEh27cwAjWpA3Q60J7NaDxzQJ         2366         604             12968   \n",
       "4  zKV8BFGy60O0q7102ALF84S6Jo5i4q         2200         586             15168   \n",
       "5  i8YiBZlrYmlkkChr5b9BUKvDO6lR1d         2126         360             17294   \n",
       "6  M3icahkfAtC9CJrtKgQ7qvyZ5SD8wC         1824         321             19118   \n",
       "7  HNzkrs9geGu1YMMfZ5Qvdt0ZaCthfB         1606         263             20724   \n",
       "8  Jxv4hxfb9zTVa5nsMDFlnjSX5LZ8MK         1366         177             22090   \n",
       "9  UhQipwcpQmiGJmScocXOGOKyCBaFUg         1209         411             23299   \n",
       "\n",
       "   eval_count_acum  \n",
       "0             1175  \n",
       "1             1818  \n",
       "2             2199  \n",
       "3             2803  \n",
       "4             3389  \n",
       "5             3749  \n",
       "6             4070  \n",
       "7             4333  \n",
       "8             4510  \n",
       "9             4921  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 workers with the highest annotation counts in the train split\n",
    "user.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top10 train_count 23299 eval_count 4921\n",
      "top40 train_count 38065 eval_count 13060\n"
     ]
    }
   ],
   "source": [
    "# the top 10 / top 40 users for training\n",
    "for n in [10, 40]:\n",
    "    np.savetxt(f\"./sup_users_top{n}.txt\", user.iloc[:n][\"worker\"].to_numpy(), fmt=\"%s\")\n",
    "    print(f\"top{n}\", \"train_count\", user.iloc[n-1][\"train_count_acum\"], \"eval_count\", user.iloc[n-1][\"eval_count_acum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate preferences\n",
    "# if preference = 0, user prefers longer summary\n",
    "# if preference = 1, user prefers shorter summary\n",
    "np.random.seed(3456)\n",
    "\n",
    "user_preference = []\n",
    "for i in range(user.shape[0]):\n",
    "    # 70% probability to have preference = 0, and 30% to have preference = 1\n",
    "    user_preference.append(np.random.choice(a=[0, 1], p=[0.7, 0.3]))\n",
    "\n",
    "user[\"preference\"] = user_preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 users\n",
      "            eval_count  train_count  eval_count%  train_count%\n",
      "preference                                                    \n",
      "0                 3371        16607    68.502337     71.277737\n",
      "1                 1550         6692    31.497663     28.722263\n",
      "\n",
      "\n",
      "top 40 users\n",
      "            eval_count  train_count  eval_count%  train_count%\n",
      "preference                                                    \n",
      "0                 8301        25821     63.56049     67.833968\n",
      "1                 4759        12244     36.43951     32.166032\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preferences of seen users in the train/validation set\n",
    "for n in [10, 40]: \n",
    "    table = pd.pivot_table(user.iloc[:n], index=\"preference\", values=[\"train_count\", \"eval_count\"], aggfunc=np.sum)\n",
    "    table[\"eval_count%\"] = 100*table[\"eval_count\"]/table[\"eval_count\"].sum()\n",
    "    table[\"train_count%\"] = 100*table[\"train_count\"]/table[\"train_count\"].sum()\n",
    "    print(f\"top {n} users\")\n",
    "    print(table)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 users\n",
      "            eval_count  eval_count%\n",
      "preference                         \n",
      "0                 7237    63.633166\n",
      "1                 4136    36.366834\n",
      "\n",
      "\n",
      "top 40 users\n",
      "            eval_count  eval_count%\n",
      "preference                         \n",
      "0                 2307    71.335807\n",
      "1                  927    28.664193\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preferences of unknown users in the validation set\n",
    "for n in [10, 40]: \n",
    "    table = pd.pivot_table(user.iloc[n:], index=\"preference\", values=[\"eval_count\"], aggfunc=np.sum)\n",
    "    table[\"eval_count%\"] = 100*table[\"eval_count\"]/table[\"eval_count\"].sum()\n",
    "    print(f\"top {n} users\")\n",
    "    print(table)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the saved sup_users_top10.txt and sup_users_preferences.txt files \n",
    "# will be used in the model training of \"Generation with Conflicting Preferences\" experiments\n",
    "# using the TL;DR dataset with synthetic preferences\n",
    "np.savetxt(\"./sup_users_preferences.txt\", user[[\"worker\", \"preference\"]].to_numpy(), fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prlhf.utils import load_openai_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46584\n",
      "16294\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# randomly sample 50 prompts from the validation set for evaluation\n",
    "train_dataset, eval_dataset, n_users = load_openai_comparisons(\n",
    "    user_file=\"./sup_users_top10.txt\", \n",
    "    sep=\"||\", \n",
    "    n_user_tokens=10,\n",
    "    max_text_length=4800, \n",
    "    sanity_check=False,\n",
    "    use_downloads=use_downloads,\n",
    "    downloads_data_path=downloads_data_path if use_downloads else None,\n",
    "    user_preference_file=\"./sup_users_preferences.txt\",\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "print(n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3617\n"
     ]
    }
   ],
   "source": [
    "unique_prompts = []  # the unique prompts in the validation set\n",
    "\n",
    "for user_prompt in eval_dataset[\"prompt\"]:\n",
    "    prompt = user_prompt.split(\"<|endoftext|>||\")[1].strip()\n",
    "    if prompt in unique_prompts:\n",
    "        continue\n",
    "    unique_prompts.append(prompt)\n",
    "\n",
    "print(len(unique_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "indices = np.random.choice(len(unique_prompts), 50, replace=False)\n",
    "\n",
    "selected_prompts = np.asarray(unique_prompts)[indices]\n",
    "print(len(selected_prompts))\n",
    "\n",
    "# this saved npy file will provide the evaluation prompts for tldr generations\n",
    "np.save(\"./tldr_selected_prompts.npy\", selected_prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
